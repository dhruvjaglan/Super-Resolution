{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDVR \n",
    "### PCD and TSA\n",
    "https://arxiv.org/pdf/1905.02716.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dconv import DeformConv2d\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pcd\n",
    " PCD alignment module aligns each neighboring frame to the reference frame\n",
    "<img src=\"PCD.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCD_Align(nn.Module):\n",
    "    \n",
    "    def __init__(self, nf=64):\n",
    "        super(PCD_Align, self).__init__()\n",
    "        \n",
    "        # L3 level3 \n",
    "        self.L3_offset_conv1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n",
    "        self.L3_offset_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.L3_dcnpack = DeformConv2d(nf, nf, 3, padding=1, stride=1,bias=True, modulation=True)\n",
    "        # L2: level 2, 1/2 spatial size\n",
    "        self.L2_offset_conv1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n",
    "        self.L2_offset_conv2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for offset\n",
    "        self.L2_offset_conv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.L2_dcnpack = DeformConv2d(nf, nf, 3, padding=1, stride=1,bias=True, modulation=True)\n",
    "        self.L2_fea_conv = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for fea\n",
    "        # L1: level 1, original spatial size\n",
    "        self.L1_offset_conv1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n",
    "        self.L1_offset_conv2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for offset\n",
    "        self.L1_offset_conv3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.L1_dcnpack = DeformConv2d(nf, nf, 3, padding=1, stride=1,bias=True, modulation=True)\n",
    "        self.L1_fea_conv = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for fea\n",
    "        # Cascading DCN\n",
    "        self.cas_offset_conv1 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)  # concat for diff\n",
    "        self.cas_offset_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "\n",
    "        self.cas_dcnpack = DeformConv2d(nf, nf, 3, padding=1, stride=1,bias=True, modulation=True)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "    def forward(self, nbr_fea_l, ref_fea_l):\n",
    "        '''align other neighboring frames to the reference frame in the feature level\n",
    "        nbr_fea_l, ref_fea_l: [L1, L2, L3], each with [B,C,H,W] features\n",
    "        '''\n",
    "        \n",
    "        # L3\n",
    "        L3_offset = torch.cat([nbr_fea_l[2], ref_fea_l[2]], dim=1)\n",
    "        L3_offset = self.lrelu(self.L3_offset_conv1(L3_offset))\n",
    "        L3_offset = self.lrelu(self.L3_offset_conv2(L3_offset))\n",
    "        L3_fea = self.lrelu(self.L3_dcnpack([nbr_fea_l[2], L3_offset]))\n",
    "        # L2\n",
    "        L2_offset = torch.cat([nbr_fea_l[1], ref_fea_l[1]], dim=1)\n",
    "        L2_offset = self.lrelu(self.L2_offset_conv1(L2_offset))\n",
    "        L3_offset = F.interpolate(L3_offset, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        L2_offset = self.lrelu(self.L2_offset_conv2(torch.cat([L2_offset, L3_offset * 2], dim=1)))\n",
    "        L2_offset = self.lrelu(self.L2_offset_conv3(L2_offset))\n",
    "        L2_fea = self.L2_dcnpack([nbr_fea_l[1], L2_offset])\n",
    "        L3_fea = F.interpolate(L3_fea, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        L2_fea = self.lrelu(self.L2_fea_conv(torch.cat([L2_fea, L3_fea], dim=1)))\n",
    "        # L1\n",
    "        L1_offset = torch.cat([nbr_fea_l[0], ref_fea_l[0]], dim=1)\n",
    "        L1_offset = self.lrelu(self.L1_offset_conv1(L1_offset))\n",
    "        L2_offset = F.interpolate(L2_offset, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        L1_offset = self.lrelu(self.L1_offset_conv2(torch.cat([L1_offset, L2_offset * 2], dim=1)))\n",
    "        L1_offset = self.lrelu(self.L1_offset_conv3(L1_offset))\n",
    "        L1_fea = self.L1_dcnpack([nbr_fea_l[0], L1_offset])\n",
    "        L2_fea = F.interpolate(L2_fea, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        L1_fea = self.L1_fea_conv(torch.cat([L1_fea, L2_fea], dim=1))\n",
    "        # Cascading\n",
    "        offset = torch.cat([L1_fea, ref_fea_l[0]], dim=1)\n",
    "        offset = self.lrelu(self.cas_offset_conv1(offset))\n",
    "        offset = self.lrelu(self.cas_offset_conv2(offset))\n",
    "        L1_fea = self.lrelu(self.cas_dcnpack([L1_fea, offset]))\n",
    "\n",
    "        return L1_fea\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSA \n",
    "The TSA fusion module fuses image information of different frames\n",
    "<img src=\"TSA.png\">,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSA_Fusion(nn.Module):\n",
    "    ''' Temporal Spatial Attention fusion module\n",
    "    Temporal: correlation;\n",
    "    Spatial: 3 pyramid levels.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nf=64, nframes=5, center=2):\n",
    "        super(TSA_Fusion, self).__init__()\n",
    "        self.center = center\n",
    "        # temporal attention (before fusion conv)\n",
    "        self.tAtt_1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.tAtt_2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "\n",
    "        # fusion conv: using 1x1 to save parameters and computation\n",
    "        self.fea_fusion = nn.Conv2d(nframes * nf, nf, 1, 1, bias=True)\n",
    "\n",
    "        # spatial attention (after fusion conv)\n",
    "        self.sAtt_1 = nn.Conv2d(nframes * nf, nf, 1, 1, bias=True)\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        self.sAtt_2 = nn.Conv2d(nf * 2, nf, 1, 1, bias=True)\n",
    "        self.sAtt_3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.sAtt_4 = nn.Conv2d(nf, nf, 1, 1, bias=True)\n",
    "        self.sAtt_5 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.sAtt_L1 = nn.Conv2d(nf, nf, 1, 1, bias=True)\n",
    "        self.sAtt_L2 = nn.Conv2d(nf * 2, nf, 3, 1, 1, bias=True)\n",
    "        self.sAtt_L3 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.sAtt_add_1 = nn.Conv2d(nf, nf, 1, 1, bias=True)\n",
    "        self.sAtt_add_2 = nn.Conv2d(nf, nf, 1, 1, bias=True)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "    def forward(self, aligned_fea):\n",
    "        B, N, C, H, W = aligned_fea.size()  # N video frames\n",
    "        #### temporal attention\n",
    "        emb_ref = self.tAtt_2(aligned_fea[:, self.center, :, :, :].clone())\n",
    "        emb = self.tAtt_1(aligned_fea.view(-1, C, H, W)).view(B, N, -1, H, W)  # [B, N, C(nf), H, W]\n",
    "\n",
    "        cor_l = []\n",
    "        for i in range(N):\n",
    "            emb_nbr = emb[:, i, :, :, :]\n",
    "            cor_tmp = torch.sum(emb_nbr * emb_ref, 1).unsqueeze(1)  # B, 1, H, W\n",
    "            cor_l.append(cor_tmp)\n",
    "        cor_prob = torch.sigmoid(torch.cat(cor_l, dim=1))  # B, N, H, W\n",
    "        cor_prob = cor_prob.unsqueeze(2).repeat(1, 1, C, 1, 1).view(B, -1, H, W)\n",
    "        aligned_fea = aligned_fea.view(B, -1, H, W) * cor_prob\n",
    "\n",
    "        #### fusion\n",
    "        fea = self.lrelu(self.fea_fusion(aligned_fea))\n",
    "\n",
    "        #### spatial attention\n",
    "        att = self.lrelu(self.sAtt_1(aligned_fea))\n",
    "        att_max = self.maxpool(att)\n",
    "        att_avg = self.avgpool(att)\n",
    "        att = self.lrelu(self.sAtt_2(torch.cat([att_max, att_avg], dim=1)))\n",
    "        # pyramid levels\n",
    "        att_L = self.lrelu(self.sAtt_L1(att))\n",
    "        att_max = self.maxpool(att_L)\n",
    "        att_avg = self.avgpool(att_L)\n",
    "        att_L = self.lrelu(self.sAtt_L2(torch.cat([att_max, att_avg], dim=1)))\n",
    "        att_L = self.lrelu(self.sAtt_L3(att_L))\n",
    "        att_L = F.interpolate(att_L, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        att = self.lrelu(self.sAtt_3(att))\n",
    "        att = att + att_L\n",
    "        att = self.lrelu(self.sAtt_4(att))\n",
    "        att = F.interpolate(att, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        att = self.sAtt_5(att)\n",
    "        att_add = self.sAtt_add_2(self.lrelu(self.sAtt_add_1(att)))\n",
    "        att = torch.sigmoid(att)\n",
    "\n",
    "        fea = fea * att * 2 + att_add\n",
    "        return fea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer(block, n_layers):\n",
    "    layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(block())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ResnetBlock_noBN(nn.Module):\n",
    "\n",
    "    def __init__(self, nf=64):\n",
    "        super(ResidualBlock_noBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "\n",
    "        # initialization\n",
    "        initialize_weights([self.conv1, self.conv2], 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = F.relu(self.conv1(x), inplace=True)\n",
    "        out = self.conv2(out)\n",
    "        return identity + out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDVR(nn.Module):\n",
    "    def __init__(self, nf=64, nframes=5, front_RBs=5, back_RBs=10, center=None,\n",
    "                 HR_in=False, w_TSA=True):\n",
    "        super(EDVR, self).__init__()\n",
    "        self.nf = nf\n",
    "        self.center = nframes // 2 if center is None else center\n",
    "        self.HR_in = True if HR_in else False\n",
    "        self.w_TSA = w_TSA\n",
    "        ResnetBlock_noBN = functools.partial(ResnetBlock_noBN, nf=nf)\n",
    "\n",
    "        if self.HR_in:\n",
    "            self.conv_first_1 = nn.Conv2d(3, nf, 3, 1, 1, bias=True)\n",
    "            self.conv_first_2 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n",
    "            self.conv_first_3 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n",
    "        else:\n",
    "            self.conv_first = nn.Conv2d(3, nf, 3, 1, 1, bias=True)\n",
    "        self.feature_extraction = make_layer(ResnetBlock_noBN, front_RBs)\n",
    "        self.fea_L2_conv1 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n",
    "        self.fea_L2_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.fea_L3_conv1 = nn.Conv2d(nf, nf, 3, 2, 1, bias=True)\n",
    "        self.fea_L3_conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "\n",
    "        self.pcd_align = PCD_Align(nf=nf)\n",
    "        if self.w_TSA:\n",
    "            self.tsa_fusion = TSA_Fusion(nf=nf, nframes=nframes, center=self.center)\n",
    "        else:\n",
    "            self.tsa_fusion = nn.Conv2d(nframes * nf, nf, 1, 1, bias=True)\n",
    "\n",
    "        #### reconstruction\n",
    "        self.recon_trunk = make_layer(ResnetBlock_noBN, back_RBs)\n",
    "        #### upsampling\n",
    "        self.upconv1 = nn.Conv2d(nf, nf * 4, 3, 1, 1, bias=True)\n",
    "        self.upconv2 = nn.Conv2d(nf, 64 * 4, 3, 1, 1, bias=True)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)\n",
    "        self.HRconv = nn.Conv2d(64, 64, 3, 1, 1, bias=True)\n",
    "        self.conv_last = nn.Conv2d(64, 3, 3, 1, 1, bias=True)\n",
    "\n",
    "        #### activation function\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C, H, W = x.size()  # N video frames\n",
    "        x_center = x[:, self.center, :, :, :].contiguous()\n",
    "\n",
    "        #### extract LR features\n",
    "        # L1\n",
    "        \n",
    "        if self.HR_in:\n",
    "            L1_fea = self.lrelu(self.conv_first_1(x.view(-1, C, H, W)))\n",
    "            L1_fea = self.lrelu(self.conv_first_2(L1_fea))\n",
    "            L1_fea = self.lrelu(self.conv_first_3(L1_fea))\n",
    "            H, W = H // 4, W // 4\n",
    "        else:\n",
    "            L1_fea = self.lrelu(self.conv_first(x.view(-1, C, H, W)))\n",
    "        L1_fea = self.feature_extraction(L1_fea)\n",
    "        # L2\n",
    "        L2_fea = self.lrelu(self.fea_L2_conv1(L1_fea))\n",
    "        L2_fea = self.lrelu(self.fea_L2_conv2(L2_fea))\n",
    "        # L3\n",
    "        L3_fea = self.lrelu(self.fea_L3_conv1(L2_fea))\n",
    "        L3_fea = self.lrelu(self.fea_L3_conv2(L3_fea))\n",
    "\n",
    "        L1_fea = L1_fea.view(B, N, -1, H, W)\n",
    "        L2_fea = L2_fea.view(B, N, -1, H // 2, W // 2)\n",
    "        L3_fea = L3_fea.view(B, N, -1, H // 4, W // 4)\n",
    "\n",
    "        #### pcd align\n",
    "        # ref feature list\n",
    "        ref_fea_l = [\n",
    "            L1_fea[:, self.center, :, :, :].clone(), L2_fea[:, self.center, :, :, :].clone(),\n",
    "            L3_fea[:, self.center, :, :, :].clone()\n",
    "        ]\n",
    "        aligned_fea = []\n",
    "        for i in range(N):\n",
    "            nbr_fea_l = [\n",
    "                L1_fea[:, i, :, :, :].clone(), L2_fea[:, i, :, :, :].clone(),\n",
    "                L3_fea[:, i, :, :, :].clone()\n",
    "            ]\n",
    "            aligned_fea.append(self.pcd_align(nbr_fea_l, ref_fea_l))\n",
    "        aligned_fea = torch.stack(aligned_fea, dim=1)  # [B, N, C, H, W]\n",
    "\n",
    "        if not self.w_TSA:\n",
    "            aligned_fea = aligned_fea.view(B, -1, H, W)\n",
    "        fea = self.tsa_fusion(aligned_fea)\n",
    "\n",
    "        out = self.recon_trunk(fea)\n",
    "        out = self.lrelu(self.pixel_shuffle(self.upconv1(out)))\n",
    "        out = self.lrelu(self.pixel_shuffle(self.upconv2(out)))\n",
    "        out = self.lrelu(self.HRconv(out))\n",
    "        out = self.conv_last(out)\n",
    "        if self.HR_in:\n",
    "            base = x_center\n",
    "        else:\n",
    "            base = F.interpolate(x_center, scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        out += base\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Perceptual loss or feature loss\n",
    "\n",
    "https://arxiv.org/abs/1603.08155\n",
    "\n",
    "<img src=\"featureloss.png\">,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(torch.nn.Module):\n",
    "    def __init__(self, resize=True):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        blocks = []\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n",
    "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n",
    "        for bl in blocks:\n",
    "            for p in bl:\n",
    "                p.requires_grad = False\n",
    "        self.blocks = torch.nn.ModuleList(blocks)\n",
    "        self.transform = torch.nn.functional.interpolate\n",
    "        self.mean = torch.nn.Parameter(torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.std = torch.nn.Parameter(torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "        self.resize = resize\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.shape[1] != 3:\n",
    "            input = input.repeat(1, 3, 1, 1)\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "        input = (input-self.mean) / self.std\n",
    "        target = (target-self.mean) / self.std\n",
    "        if self.resize:\n",
    "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
    "        loss = 0.0\n",
    "        x = input\n",
    "        y = target\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            y = block(y)\n",
    "            loss += torch.nn.functional.l1_loss(x, y)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
